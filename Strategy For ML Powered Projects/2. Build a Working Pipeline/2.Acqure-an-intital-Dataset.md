# Acquire an initial Dataset

## Iterate on Datasets
We need so see data only as a tool, that we can improve as well

Different workflows for researchers and industry
![Difference researchers industry](./img/diff-researchers-industry.png)

## Do Data Science

ML models are no magic trick, they can extract trends and pattern from data on the same
way as humans can.
This means, you need to understand and visualize correlations yourself, be increase the success rate of
your model significantly.

## Be Efficient, Start Small

* Use only a small sized dataset to start building your model.
Only when you have a working strategy increase you sample size.

* when you have Terrabytes of data you can try to use as much data that fits on your local machines memory

## Insights Versus Products

Also we need to make sure, that the correlations are visible in newly collected data as well
as that such data is still available in the future

1. **Check for dataset quality**
2. **Check for predicting trends**

## A Data Quality Rubric

 ! Each dataset has its own biases and oddities, which we have to understand to work with the dataset
 properly:
 
 ### Dataformat
 Is the dataset already formatted in such a way that you have clear inputs and outputs or
 does it require additional preprocessing and labeling?
 
 If the dataset is already pre processed and you are pleased with the work, you should still try
 to replicate the values that are processed or aggregated
 
 ### Data quality
 
 * Check for missing values in key features (if there are to many values missing, change dataset)
 * values can be missing, imprecise or corrupted, therefore you should inspect each column for these errors
    * Is the text faulty or inconsistent?
    * Is the img clear enough to do the task yourself?
    * Which proportion of your data seems noisy or incorrect?
    * if the data has labels do you agree with them 
    
### Data quantity and distribution
How much data is accessible?
Is the dataset to small?
Are columns underrepresented (to many missing values, for example)

#### Table of data quality

| Quality | Format | Quantity and distribution |
|----|----|----|
| Are any important fields ever empty? | How many preprocessing steps does your data require? | How many examples do you have?
| Are there potential errors of measurement? | Will you be able to preprocess in the same way in production? | how many examples per class? Are any absent?

often it is easier to generate more data from the existing data, than to optimize the model itself

You should preprocess your data separate from model training, because it can slow down the optimizations significantly.

 