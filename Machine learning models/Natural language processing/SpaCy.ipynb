{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SpaCy\n",
    "\n",
    "SpaCy tries to minimize the time you need to create an nlp algorithm.\n",
    "To do this SpaCy provides classes of specific languages that contain the whole processing pipeline already.\n",
    "The only thing you need to do, is to deactivate the steps in the pipeline, you don't need and to optimize the algorithm for\n",
    "your specific dataset.\n",
    "\n",
    "Below is a simple nlp algorithm, that uses SpaCy\n",
    "\n",
    "(This is a altered version of the tutorial Ines Montani did [here](https://course.spacy.io/en/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import the English language class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = English()\n",
    "\n",
    "# Created by processing a string of text with the nlp object\n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "# Iterate over tokens in a Doc\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "\n",
    "# Or get a specific token and print its text\n",
    "print (doc[1].text)\n",
    "\n",
    "# Different token attributes\n",
    "print(\"Index:   \", [token.i for token in doc])\n",
    "print(\"Text:    \", [token.text for token in doc])\n",
    "print(\"is_alpha:\", [token.is_alpha for token in doc])\n",
    "print(\"is_punct:\", [token.is_punct for token in doc])\n",
    "print(\"like_num:\", [token.like_num for token in doc])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have seen a basic workflow we can dive deeper into SpaCys' features:\n",
    "\n",
    "## Statistical models\n",
    "\n",
    "Statistical models are used to predict linguistic attributes in context\n",
    "* Part-of-speech tags\n",
    "* Syntactic dependencies\n",
    "* Named entities\n",
    "\n",
    "This means SpaCy train algorithms on specific texts (on be retrained/updated to increase accuracy in project) to detect in what\n",
    "context the word is and what type of word it is.\n",
    "This is especial useful later on, when we build queries to look for specific sentences in text.\n",
    "\n",
    "To use the models we need to install them first:\n",
    "\n",
    "```\n",
    "$ python -m spacy download en_core_web_sm\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process text\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # print text,  the according part-of-speech tag, the relation in a sentence and the parent token\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also predict named entities, which are specific descriptions of elements in a sentence:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Process text\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for 1billion euro\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To understand the definitions better spacy has the explain method, that illustrates, what a tag means"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spacy.explain(\"GPE\")\n",
    "\n",
    "spacy.explain(\"NNP\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Match patterns\n",
    "With pattern matching we can filter out specific sentences of parts of a given text.\n",
    "\n",
    "To implement this we can create a list of dictionaries, that describes what\n",
    "the pattern should look like:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Match exact token texts\n",
    "[{\"TEXT\": \"iPhone\"}, {\"TEXT\", \"X\"}]\n",
    "\n",
    "# Match lexical attributes (iphone x should be detected in any form of writing it)\n",
    "[{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "\n",
    "# Match any token attributes (Lemma is the stem of a word and after that should follow a noun)\n",
    "[{\"LEMMA\": \"buy\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Matchea can be quantified with the op keyword (\"OP\": ? - 0 or 1, ! - 0, + - 1 or more, * - 0 or more)\n",
    "[{\"LEMMA\": \"buy\"}, {\"POS\": \"DET\", \"OP\":\"?\"}, {\"POS\": \"NOUN\"}]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Result will be something with stem of buy then an optional article and finally a noun.\n",
    "\n",
    "### Example Matcher"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Import Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", None, pattern)\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    # Get the matched span\n",
    "    print(\"Matched span:\", span.text)\n",
    "    # Get the span's root token (decides category of the phrase)\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    # And root head token (The syntactic parent that governs the phrase)\n",
    "    print(\"Root head token:\", span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Phrase matcher\n",
    "PhraseMatcher is more efficient than Matcher, because it takes whole Doc objects as patterns\n",
    "\n",
    "#### Example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add(\"DOG\", None, pattern)\n",
    "doc = nlp(\"I hav a Golden Retriever\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Shared vocab and string store\n",
    "\n",
    "SpaCy saves every word with its given hash value (and more) as a \"Lexeme\" in the string store via \"nlp.vocab.strings\".\n",
    "Internally SpaCy only uses the given hash values to save memory\n",
    "\n",
    "Lexemes saves\n",
    "* Text\n",
    "* Hash-id\n",
    "* attributes like (lexeme.is_alpha)\n",
    "\n",
    "Note: if a string doesn't exists in the string store, we can't call its hash value to get it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get hash value or text\n",
    "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "\n",
    "# Other way to get the information\n",
    "doc = nlp(\"I love coffee\")\n",
    "print(doc.vocab.strings[\"coffee\"])\n",
    "\n",
    "# Get the lexeme itself and show its information\n",
    "lexeme = nlp.vocab[\"coffee\"]\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Doc Data Structure\n",
    "\n",
    "The doc class stores 3 vales\n",
    "* Words: a list of every word in a string\n",
    "* Spaces: stores if after a word comes a space or not\n",
    "* vocab: contains the shard word class\n",
    "\n",
    "### Span\n",
    "The Span is a slice of the doc and it takes 3 arguments:\n",
    "* doc: which doc it refers to\n",
    "* start: contains the index of the start token\n",
    "* end: contains the index of the end token (Span doesn't contain the element in the end index)\n",
    "\n",
    "With that knowledge we can define own named entities in form of spans and can add them to the doc.ents to find specific\n",
    "patterns in an text.\n",
    "\n",
    "Good to know:\n",
    "* Use token attributes when possible and convert tokens to strings as late as possible, otherwise the performance of your\n",
    "    project will sink dramatically\n",
    "* Don't forget to pass in the shared vocab when creating the Doc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# Create doc manually\n",
    "doc = Doc(nlp.vocab, words=[\"hello\", \"world\", \"!\"], spaces=[True, False, False])\n",
    "\n",
    "# Create span manually (with label)\n",
    "span = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word vectors and semantic similarity\n",
    "\n",
    "Because SpaCy stores additional data in every word token, it allows you to compare to objects and predict similarity.\n",
    "* Doc.similarity()\n",
    "* Span.similarity()\n",
    "* Token.similarity()\n",
    "\n",
    "**Important**: only the medium and large models store enough data to use this feature!!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load medium/large model (they contain the needed vectors)\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Compare two documents / span / token\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Combining models and rules\n",
    "When using rules to gather information from data, SpaCy offers two types:\n",
    "\n",
    "| Â´ | Statistical models | Rule-based systems |\n",
    "|----|----|----|\n",
    "| Use cases | application needs to generalize based on examples | | dictionary with finite number of examples\n",
    "| Real-world examples | product names, person names, subject/object relationships | countries of the world, cities, drug names, dog breeds |\n",
    "| spaCy features | entity recognizer, dependency parser, part-of-speech tagger | tokenizer, Matcher, PhraseMatcher |\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing pipelines\n",
    "\n",
    "As mentioned earlier SpaCy's nlp method already provides you with a whole nlp pipeline and it looks like this:\n",
    "\n",
    "| Name | Description | Creates |\n",
    "|---|---|---|\n",
    "| 1. tokenizer | Turn text in Doc object | Doc |\n",
    "| 2. tagger | Part-of.speech tagger | Token.tag, token.pos |\n",
    "| 3. parser | Dependency parser | Token.dep, Token.head, Doc.sents, Doc.noun_chunks |\n",
    "| 4. ner | Named entity recognizer | Doc.ents, Token.ent_iob, token.ent_type |\n",
    "| 5. textcat | text classifier | Doc.cats |\n",
    "\n",
    "We can check which pipelines are applied with the following commands:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "print(nlp.pipeline)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom pipeline components\n",
    "You can create custom pipelines steps like this:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def custom_component(doc):\n",
    "    # Do something to the doc here\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(custom_component)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In addition you can decide where the component should be executed\n",
    "\n",
    "| Argument | Description | Example |\n",
    "|---|---|---|\n",
    "| last | If True, add last | nlp.add_pipe(component, last=True) |\n",
    "| first | If True, add first | nlp.add_pipe(component, first=True) |\n",
    "| before | Add before component | nlp.add_pipe(component, before=\"ner\") |\n",
    "| after | Add after component | nlp.add_pipe(component, after=\"tagger\") |\n",
    "\n",
    "## Custom attributes\n",
    "Custom metadata can be added to docs, token and spans or changed via the \"._\" property\n",
    "\n",
    "We can create extensions as well, there are 3 types:\n",
    "1. Attribute extensions: Set a default value that can be overwritten\n",
    "2. Property extensions: Defining a getter and an optional setter function. Getter only called when attribute value is called\n",
    "3. Method extensions: makes the extension attribute a callable method (you can pass multiple arguments to it)\n",
    "\n",
    "**Important** Attributes for Span should always be handled with a getter function!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "# Set extensions on the global Doc, token or Span via\n",
    "Doc.set_extension(\"title\", default=None)\n",
    "Token.set_extension(\"is_color\", default=False)\n",
    "Span.set_extension(\"has_color\", default=False)\n",
    "\n",
    "# change the attributes\n",
    "doc._.title = \"my document\"\n",
    "token._.is_color = True\n",
    "\n",
    "\n",
    "# Attribute extension\n",
    "Token.set_extension(\"is_color\", default=False)\n",
    "\n",
    "doc = nlp(\"Water is not green.\")\n",
    "\n",
    "# Overwrite attribute extension\n",
    "doc[3]._.is_color = True\n",
    "\n",
    "\n",
    "# Property extension\n",
    "# Define getter function\n",
    "def get_is_color(token):\n",
    "    color = [\"red\", \"yellow\", \"blue\"]\n",
    "    return token.text in color\n",
    "\n",
    "# Set extension on the token with getter\n",
    "Token.set_extension(\"is_color\", getter=get_is_color)\n",
    "\n",
    "print(doc[3]._.is_color, \"-\", doc[3].text)\n",
    "\n",
    "\n",
    "# Method extension\n",
    "# Define a Method with arguments\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Set extension on the Doc\n",
    "Doc.set_extension(\"has_token\", method=has_token)\n",
    "print(\"Has blue: \", doc._.has_token(\"blue\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scaling and performance\n",
    "* Use nlp.pip method, because this way text a processed as stream, that means Asynchronous\n",
    "* Much faster than nlp on each text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LOTS_OF_TEXTS = None\n",
    "\n",
    "# BAD\n",
    "docs = [nlp(text) for text in LOTS_OF_TEXTS]\n",
    "\n",
    "# GOOD\n",
    "docs = list(nlp.pipe(LOTS_OF_TEXTS))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With nlp.pipe you can add an get context to each text (Useful for associating metadata with the doc):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"This is a text\", {\"id\": 1, \"page_number\":15}),\n",
    "    (\"And another text\", {\"id\": 2, \"page_number\":16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context[\"page_number\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another way to improve the processing speed, is to deactivate unnecessary pipeline steps:\n",
    "* we can use the \"nlp.make_doc\" method to only run the tokenizer\n",
    "* and disable specific pipeline components with \"nlp.disable_pipes\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# only tokenizer\n",
    "doc = nlp.make_doc(\"Hello world!\")\n",
    "\n",
    "# disable tagger and parser\n",
    "with nlp.disable_pipes(\"tagger\", \"parser\"):\n",
    "    # Process the text and print the entities\n",
    "    doc = nlp(\"Hello World!\")\n",
    "    print(doc.ents)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train and updating models\n",
    "Why updating the model?\n",
    "* Better results on specific domains\n",
    "* Learn classification schemes specifically for your problem\n",
    "* Essential for text classification\n",
    "* Very useful for named entity recognition\n",
    "* Less critical for part-of-speech tagging and dependency parsing\n",
    "\n",
    "How to train\n",
    "1. **Initialize** the model weights randomly with nlp.begin_training\n",
    "2. **Predict** a few examples with the current weights by calling nlp.update\n",
    "3. **Compare** prediction with true labels\n",
    "4. **Calculate** how to change weights to improve predictions\n",
    "5. **Update** weights slightly\n",
    "6. Go back to 2\n",
    "\n",
    "The steps of a training loop\n",
    "1. **Loop** for a number of times\n",
    "2. **Shuffle the training data\n",
    "3. **Divide** the data into batches\n",
    "4. **Update** the model for each batch\n",
    "5. **Save** the updated model\n",
    "\n",
    "To label data fast on your own, use [brat](http://brat.nlplab.org/)\n",
    "\n",
    "#### Example train loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "TRAINING_DATA = [\n",
    "    (\"How to preorder the iPhone X\", {\"entities\": [(20, 28, \"GADGET\")]})\n",
    "    # more examples...\n",
    "]\n",
    "nlp.begin_training\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for i in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    # Create batches and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA):\n",
    "        # Split the batch in texts and annotations\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations)\n",
    "\n",
    "# Save the model\n",
    "# nlp.to_disk(path_to_model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Update an existing model\n",
    "* Improve the predictions on new data\n",
    "* useful for existing categories like \"PERSON\"\n",
    "* Also possible to add new categories\n",
    "* Be careful, the model can forget the old categories!!\n",
    "\n",
    "#### Example Setting up a new pipeline from scratch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# start with blank English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create blank entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "# Add a new label\n",
    "ner.add_label(\"GADGET\")\n",
    "\n",
    "# Start training\n",
    "# Loop for 10 iterations\n",
    "for i in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    # Create batches and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        # Split the batch in texts and annotations\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Problems\n",
    "\n",
    "#### Models can \"forget\" things\n",
    "Don't update models with only one category or the model will unlearn the other categories"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# BAD\n",
    "TRAINING_DATA = [\n",
    "    (\"reddit is a website\", {\"entities\": [(0, 6, \"WEBSITE\")]})\n",
    "]\n",
    "\n",
    "# GOOD\n",
    "TRAINING_DATA = [\n",
    "    (\"reddit is a website\", {\"entities\": [(0, 6, \"WEBSITE\")]}),\n",
    "    (\"Obama is a person\", {\"entitres\": [(0, 5, \"PERSON\")]})\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Models can't learn everything\n",
    "* SpaCy's models make predictions based on local context\n",
    "* Model can struggle to learn if decision is difficult to make based on context\n",
    "* Labels need to be consistent and not to0 specific\n",
    "    * For example: \"CLOTHING\" is better than \"ADULT_CLOTHING\" and \"CHILDRENS_CLOTHING\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "* https://course.spacy.io/en/\n",
    "* https://www.youtube.com/watch?v=DBbBRwpneLs&feature=emb_logo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}