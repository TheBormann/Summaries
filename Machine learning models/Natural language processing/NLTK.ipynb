{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Nltk library\n",
    "\n",
    "Natural language processing (NLP) is a way of transforming text into key representation of that text, which the computer\n",
    "can work with more easily.\n",
    "\n",
    "The natural language toolkit (NLTK) is the most popular library to do so in python."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installation\n",
    "Install it with anaconda or with pip using\n",
    "```\n",
    "conda install nltk\n",
    "# or\n",
    "pip install nltk\n",
    "```\n",
    "then to install all NLTK packages needed, run the code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "# nltk.download('all')\n",
    "\n",
    "example_text = 'There is nothing bigger or older than the universe. The questions I would like to talk about are: one, where did we come from? How did the universe come into being? Are we alone in the universe? Is there alien life out there? What is the future of the human race?' \\\n",
    "               'Up until the 1920s, everyone thought the universe was essentially static and unchanging in time. Then it was discovered that the universe was expanding. Distant galaxies were moving away from us. This meant they must have been closer together in the past. If we extrapolate back, we find they must have all been on top of each other about 15 billion years ago. This was the Big Bang, the beginning of the universe.' \\\n",
    "               'But was there anything before the Big Bang? If not, what created the universe? Why did the universe emerge from the Big Bang the way it did? We used to think that the theory of the universe could be divided into two parts. First, there were the laws like Maxwell’s equations and general relativity that determined the evolution of the universe, given its state over all of space at one time. And second, there was no question of the initial state of the universe.' \\\n",
    "               'We have made good progress on the first part, and now have the knowledge of the laws of evolution in all but the most extreme conditions. But until recently, we have had little idea about the initial conditions for the universe. However, this division into laws of evolution and initial conditions depends on time and space being separate and distinct. Under extreme conditions, general relativity and quantum theory allow time to behave like another dimension of space. This removes the distinction between time and space, and means the laws of evolution can also determine the initial state. The universe can spontaneously create itself out of nothing.'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This will open the downloader an will allow you to install all packages you need for sentiment analysis, that aren't installed already.\n",
    "Select all and install them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess Text\n",
    "Terminology used in NLP:\n",
    "* corpora: body of text (nltk.corpus provides sample text to train tokenizer on)\n",
    "* lexicon: like a dictionary, describes the meaning of words (for example sentiment, dialects, words used differently in other fields)\n",
    "* Tokens: elements (words, sentences) of and (sentence, text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenize Text\n",
    "First we need to convert the text in to \"tokens\", this means breaking up the text into an list of:\n",
    " * sentences and/or\n",
    " * list of words."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'is', 'nothing', 'bigger', 'or', 'older', 'than', 'the', 'universe', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def preprocess_input(text):\n",
    "    \"\"\"\n",
    "    :param text: text\n",
    "    :return: Text tokenized\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return tokens\n",
    "\n",
    "tokenized = preprocess_input(example_text)\n",
    "print(tokenized[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is possible to train the tokenizer yourself, this is quite handy if the text you want to analyse has a specific 'style'\n",
    "that the normal tokenizer can't pick up correctly"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized_custom = custom_sent_tokenizer.tokenize(sample_text)\n",
    "tokenized_custom_word = [nltk.word_tokenize(i) for i in tokenized_custom]\n",
    "print(tokenized_custom_word[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRESIDENT', 'GEORGE', 'W.', 'BUSH', \"'S\", 'ADDRESS', 'BEFORE', 'A', 'JOINT', 'SESSION', 'OF', 'THE', 'CONGRESS', 'ON', 'THE', 'STATE', 'OF', 'THE', 'UNION', 'January', '31', ',', '2006', 'THE', 'PRESIDENT', ':', 'Thank', 'you', 'all', '.']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stop Words\n",
    "Stop words are words, that mislead or don't help in your data analysis, so you want to filter these ones out before moving on."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'nothing', 'bigger', 'older', 'universe', '.', 'The', 'questions', 'I', 'would', 'like', 'talk', ':', 'one', ',', 'come', '?', 'How', 'universe', 'come', '?', 'Are', 'alone', 'universe', '?', 'Is', 'alien', 'life', '?', 'What', 'future', 'human', 'race', '?', 'Up', '1920s', ',', 'everyone', 'thought', 'universe', 'essentially', 'static', 'unchanging', 'time', '.', 'Then', 'discovered', 'universe', 'expanding', '.', 'Distant', 'galaxies', 'moving', 'away', 'us', '.', 'This', 'meant', 'must', 'closer', 'together', 'past', '.', 'If', 'extrapolate', 'back', ',', 'find', 'must', 'top', '15', 'billion', 'years', 'ago', '.', 'This', 'Big', 'Bang', ',', 'beginning', 'universe.But', 'anything', 'Big', 'Bang', '?', 'If', ',', 'created', 'universe', '?', 'Why', 'universe', 'emerge', 'Big', 'Bang', 'way', '?', 'We', 'used', 'think', 'theory', 'universe', 'could', 'divided', 'two', 'parts', '.', 'First', ',', 'laws', 'like', 'Maxwell', '’', 'equations', 'general', 'relativity', 'determined', 'evolution', 'universe', ',', 'given', 'state', 'space', 'one', 'time', '.', 'And', 'second', ',', 'question', 'initial', 'state', 'universe.We', 'made', 'good', 'progress', 'first', 'part', ',', 'knowledge', 'laws', 'evolution', 'extreme', 'conditions', '.', 'But', 'recently', ',', 'little', 'idea', 'initial', 'conditions', 'universe', '.', 'However', ',', 'division', 'laws', 'evolution', 'initial', 'conditions', 'depends', 'time', 'space', 'separate', 'distinct', '.', 'Under', 'extreme', 'conditions', ',', 'general', 'relativity', 'quantum', 'theory', 'allow', 'time', 'behave', 'like', 'another', 'dimension', 'space', '.', 'This', 'removes', 'distinction', 'time', 'space', ',', 'means', 'laws', 'evolution', 'also', 'determine', 'initial', 'state', '.', 'The', 'universe', 'spontaneously', 'create', 'nothing', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "words = word_tokenize(example_text)\n",
    "filtered_sentences = [w for w in words if not w in stopwords]\n",
    "print(filtered_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stemming\n",
    "The process of stemming means, to cut every word to the words stem, to remove double values.\n",
    "For example \"walking\", \"walk\", \"walked\" = \"walk\"\n",
    "This makes it easier to cluster words together and get an overall overview of the text.\n",
    "#### IMPORTANT: nltk does it automatically for you if you are using WordNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'is', 'noth', 'bigger', 'or', 'older', 'than', 'the', 'univers', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "words = word_tokenize(example_text)\n",
    "stem_words = [ps.stem(w) for w in words]\n",
    "print(stem_words[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part of Speech Tagging\n",
    "This labels every word in a sentence or text it respective part in the sentence (I know the header says it all)\n",
    "It will label words as nouns, adjective, verb, ... and even covers tenses of sentences\n",
    "\n",
    "[POS tag list:](https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/)\n",
    "\n",
    "* CC\tcoordinating conjunction\n",
    "* CD\tcardinal digit\n",
    "* DT\tdeterminer\n",
    "* EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "* FW\tforeign word\n",
    "* IN\tpreposition/subordinating conjunction\n",
    "* JJ\tadjective\t'big'\n",
    "* JJR\tadjective, comparative\t'bigger'\n",
    "* JJS\tadjective, superlative\t'biggest'\n",
    "* LS\tlist marker\t1)\n",
    "* MD\tmodal\tcould, will\n",
    "* NN\tnoun, singular 'desk'\n",
    "* NNS\tnoun plural\t'desks'\n",
    "* NNP\tproper noun, singular\t'Harrison'\n",
    "* NNPS\tproper noun, plural\t'Americans'\n",
    "* PDT\tpredeterminer\t'all the kids'\n",
    "* POS\tpossessive ending\tparent\\'s\n",
    "* PRP\tpersonal pronoun\tI, he, she\n",
    "* PRP$\tpossessive pronoun\tmy, his, hers\n",
    "* RB\tadverb\tvery, silently,\n",
    "* RBR\tadverb, comparative\tbetter\n",
    "* RBS\tadverb, superlative\tbest\n",
    "* RP\tparticle\tgive up\n",
    "* TO\tto\tgo 'to' the store.\n",
    "* UH\tinterjection\terrrrrrrrm\n",
    "* VB\tverb, base form\ttake\n",
    "* VBD\tverb, past tense\ttook\n",
    "* VBG\tverb, gerund/present participle\ttaking\n",
    "* VBN\tverb, past participle\ttaken\n",
    "* VBP\tverb, sing. present, non-3d\ttake\n",
    "* VBZ\tverb, 3rd person sing. present\ttakes\n",
    "* WDT\twh-determiner\twhich\n",
    "* WP\twh-pronoun\twho, what\n",
    "* WP$\tpossessive wh-pronoun\twhose\n",
    "* WRB\twh-abverb\twhere, when"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "def part_of_speech_gen(tokenized_text):\n",
    "    \"\"\"\n",
    "    This function converts tokenized text into a list of word and part of speech pairs.\n",
    "    :param tokenized_text: Already word tokenized text\n",
    "    :return: word and part of speech pairs put into an list or -1 if something went wrong\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return [nltk.pos_tag(i)\n",
    "                for i in tokenized_text]\n",
    "\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "part_of_speech = part_of_speech_gen(tokenized_custom_word)\n",
    "print(part_of_speech[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chunking\n",
    "After labeling every word as a part of speech, we can use this to group words together into chunks.\n",
    "The main idea is, to group these words into \"noun phrases\", put verbs, adverbs and co that describe a specific noun with\n",
    "the noun together.\n",
    "\n",
    "To do this we are using regular expressions and speech tags"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  There/EX\n",
      "  is/VBZ\n",
      "  (NP nothing/NN)\n",
      "  bigger/JJR\n",
      "  or/CC\n",
      "  older/JJR\n",
      "  than/IN\n",
      "  (NP the/DT universe/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "def chunking_text(tokenized_text):\n",
    "    \"\"\"\n",
    "    This function chunks sentences of a text and saves the chunk-trees in a list\n",
    "    :param tokenized_text: Already sentence tokenized text\n",
    "    :return: list of nlt.Tree class object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tags = part_of_speech_gen(tokenized_text)\n",
    "        # chunking examples\n",
    "        # chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?} \"\"\"\n",
    "        chunkGram = r\"\"\"\n",
    "          NP: {<DT|PP\\$>?<JJ>*<NN>}     # chunk determiner/possessive, adjectives and noun\n",
    "          {<NNP>+}                      # chunk sequences of proper nouns\n",
    "          \"\"\"\n",
    "        chunkParser = nltk.RegexpParser(chunkGram)\n",
    "        return [chunkParser.parse(tag) for tag in tags]\n",
    "    except:\n",
    "        return -1\n",
    "chunked = chunking_text(tokenized)\n",
    "print(chunked[0])\n",
    "chunked[5].draw()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chinking\n",
    "Chinking is the removal of elements from a chunk, so it's part of the chunking process.\n",
    "A chink i specific is an element we want to remove from the chunk.\n",
    "\n",
    "#### Chinking and chunking is performed similary, the chunk gramatic defines what should be performed:\n",
    "* {} = Chunk\n",
    "* }{ = Chink"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk There/EX)\n",
      "  is/VBZ\n",
      "  (Chunk nothing/NN bigger/JJR or/CC older/JJR)\n",
      "  than/IN\n",
      "  the/DT\n",
      "  (Chunk universe/NN ./.))\n"
     ]
    }
   ],
   "source": [
    "def chink_text(tokenized_text):\n",
    "    \"\"\"\n",
    "    This function removes parts of a chunks\n",
    "    :param tokenized_tex: Already sentence tokenized text\n",
    "    :return: list of nlt.Tree class object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tags = part_of_speech_gen(tokenized_text)\n",
    "        # Chinking examples\n",
    "        chunkGram = r\"\"\"\n",
    "          Chunk: {<.*>+}     # chunk all\n",
    "          }<VB.?|IN|DT>+{    # chink\n",
    "          \"\"\"\n",
    "        chunkParser = nltk.RegexpParser(chunkGram)\n",
    "        return [chunkParser.parse(tag) for tag in tags]\n",
    "    except:\n",
    "        return -1\n",
    "chink = chink_text(tokenized)\n",
    "print(chink[0])\n",
    "chink[5].draw()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Named Entity Recognition\n",
    "Named entity will recognize specific types of of things in a text, for [example](https://pythonprogramming.net/named-entity-recognition-nltk-tutorial/):\n",
    "\n",
    "ORGANIZATION - Georgia-Pacific Corp., WHO\n",
    "PERSON - Eddy Bonte, President Obama\n",
    "LOCATION - Murray River, Mount Everest\n",
    "DATE - June, 2008-06-29\n",
    "TIME - two fifty a m, 1:30 p.m.\n",
    "MONEY - 175 million Canadian Dollars, GBP 10.40\n",
    "PERCENT - twenty pct, 18.75 %\n",
    "FACILITY - Washington Monument, Stonehenge\n",
    "GPE - South East Asia, Midlothian\n",
    "\n",
    "The problem with this solution, is that it is prone to false positives"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "named_entity = [nltk.ne_chunk(sen, binary=True) for sen in part_of_speech]\n",
    "named_entity[2].draw()\n",
    "\n",
    "# short workflow\n",
    "test = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(example_text)), binary=False)\n",
    "test.draw()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lemmatizing\n",
    "Lemmatizing is like Stemming, but trims words to the actual stem and not some arbitrary stem stemming provides."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'meant', 'they', 'must', 'have', 'been', 'closer', 'together', 'in', 'the', 'past', '.'] \n",
      " ['This', 'meant', 'they', 'must', 'have', 'been', 'close', 'together', 'in', 'the', 'past', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(token, pos=\"a\") for token in tokenized[8]]\n",
    "print(tokenized[8], \"\\n\", lemmatized_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NLTK Corpora\n",
    "Corpora are Databases to train you nlp algorithm with.\n",
    "Find where the nltk module is installed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\envs\\ML\\lib\\site-packages\\nltk\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(nltk.__file__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can open the data.py file and find links to where our nltk dataset are stored.\n",
    "Under windows it is under \"appdata/nltk\"\n",
    "\n",
    "In this folder there are many different dataset that cover a wide range of topics (form Chat, to movie rating)\n",
    "And import it into your python project:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The Tragedie of Julius Caesar by William Shakespeare \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "sample = gutenberg.raw(\"shakespeare-caesar.txt\")\n",
    "print(sample[:54])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### WordNet\n",
    "\n",
    "Wordnet is a Corpora that we can use in nltk, but is in contrast to classic dataset provided by other corporas, WordNet\n",
    "is more like a Lexicon and lets us look up **words**, their meaning, synonyms or antonyms.\n",
    "\n",
    "To work with wordnet we need to create a synset, which looks the word up in WordNet and lets us see the aspects, like meaning and synonyms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank.n.01\n",
      "bank\n",
      "sloping land (especially the slope beside a body of water)\n",
      "['they pulled the canoe up on the bank', 'he sat on the bank of the river and watched the currents']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Find synsets of \"Bank\"\n",
    "syns = wordnet.synsets(\"Bank\")\n",
    "\n",
    "# Get the synset\n",
    "print(syns[0].name())\n",
    "\n",
    "# Just the word\n",
    "print(syns[0].lemmas()[0].name())\n",
    "\n",
    "# Definiton\n",
    "print(syns[0].definition())\n",
    "\n",
    "# Examples\n",
    "print(syns[0].examples())\n",
    "\n",
    "# Synonyms and antonyms\n",
    "synonyms ="
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting insight\n",
    "\n",
    "\n",
    "```\n",
    "but_and_usage = sum(\n",
    "    (count_word_usage(tokens, [\"but\", \"and\"]) for tokens in sentences)\n",
    ")\n",
    "```\n",
    "\n",
    "## Creating features\n",
    "With the tokenized text we can filter out certain elements of the text, that are not relevant for us.\\\n",
    "For example remove \"but\" and \"and\":\n",
    "```\n",
    "but_and_usage = sum(\n",
    "    (count_word_usage(tokens, [\"but\", \"and\"]) for tokens in sentences)\n",
    ")\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# UNDER CONSTRUCTION\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "* https://www.youtube.com/watch?v=FLZvOKSCkxY&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}