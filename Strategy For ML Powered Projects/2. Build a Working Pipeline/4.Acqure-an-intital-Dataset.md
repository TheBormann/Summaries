# Acquire an initial Dataset

## Iterate on Datasets
We need so see data only as a tool, that we can improve as well

Different workflows for researchers and industry
![Difference researchers industry](./img/diff-researchers-industry.png)

## Do Data Science

ML models are no magic trick, they can extract trends and pattern from data on the same
way as humans can.
This means, you need to understand and visualize correlations yourself, be increase the success rate of
your model significantly.

## Be Efficient, Start Small

* Use only a small sized dataset to start building your model.
Only when you have a working strategy increase you sample size.

* when you have Terrabytes of data you can try to use as much data that fits on your local machines memory

## Insights Versus Products

Also we need to make sure, that the correlations are visible in newly collected data as well
as that such data is still available in the future

1. **Check for dataset quality**
2. **Check for predicting trends**

## A Data Quality Rubric

 ! Each dataset has its own biases and oddities, which we have to understand to work with the dataset
 properly:
 
 ### Dataformat
 Is the dataset already formatted in such a way that you have clear inputs and outputs or
 does it require additional preprocessing and labeling?
 
 If the dataset is already pre processed and you are pleased with the work, you should still try
 to replicate the values that are processed or aggregated
 
 ### Data quality
 
 * Check for missing values in key features (if there are to many values missing, change dataset)
 * values can be missing, imprecise or corrupted, therefore you should inspect each column for these errors
    * Is the text faulty or inconsistent?
    * Is the img clear enough to do the task yourself?
    * Which proportion of your data seems noisy or incorrect?
    * if the data has labels do you agree with them 
    
### Data quantity and distribution
How much data is accessible?
Is the dataset to small?
Are columns underrepresented (to many missing values, for example)

#### Table of data quality

| Quality | Format | Quantity and distribution |
|----|----|----|
| Are any important fields ever empty? | How many preprocessing steps does your data require? | How many examples do you have?
| Are there potential errors of measurement? | Will you be able to preprocess in the same way in production? | how many examples per class? Are any absent?

often it is easier to generate more data from the existing data, than to optimize the model itself

You should preprocess your data separate from model training, because it can slow down the optimizations significantly.

## Label to Find Data Trends
In the following step we will try to imagine, what kind of structure the model will learn.

### Summary Statistics
Summary statistics are a great starting point to understand a model.  
Most often Data-scientists use the distribution between classes of the data, to get insight.
This helps to prevent overestimation of the models performance. (This can happen, when the model is leveraging only one particularly informative feature)

### Explorer and Label Efficiently
Descriptive statistics only get you some understanding, so it's highly advised to look at individual data points as well.  
A efficient way to do is, with the use of Clustering. A clustering algorithm groups the data together by measuring the distance
between data.

This look out for when creating clusters:
* How many clusters are forming?
* What are differences between clusters?
* Does all clusters have the same density? If not it's hard for the model, to get the weaker populated features.
* How **hard** is it to understand the cluster-forming? If it's hard, these could be the first starting points to improve the 
models performance.

#### Vectorizing
Vectorizing means to turn categorical data into numerical data in a meaningful way and to normalize numerical data.
This is necessary for most algorithms to pick patterns up more easily.

##### Tabular data
* Vectorized categorical data should not be vectorized in a continuous way! (Use one hot encoding)
* Continuous features should be normalized (To help get features with a small scale the same impact as ones with large scale)
A good way to do this is, to set the mean zero and the variance to one
* more complex features should be broken down to the key characteristics. (For dates could it be, if a day is a week day or not)
* If some categories only have a view data points, it's useful to group them together

##### Avoid Data Leakage
Never use both your validation and training data for normalization or category selection, because you would be leveraging information from
outside the training data, that would end in an overfitted model.

##### Text data
Simplest way is to use a count vector, is one hot encoding for every word unique word in the text. Because it
ignores the order of words it's a "bag of words" method.

#### Image data
An image is already vectorized

Example on how it implement them in code, are available on page 77

##### Transfer Learning
Transfer Learning means, that a pretrained model can be trained further with additional data, to learn new features.

#### Dimensionality reduction and Clustering
How to understand higher dimensional data:
"To deal with hyper-planes in a 14-dimensional space, visualize a 3D space and say fourteen to yourself very loudly.
Everyone does it." ~ Geoffrey Hinton

The following dimensionality reduction techniques allow you to visualize higher dimensional data onto a 2D plane:
* t-SNE
* UMAP

If you see clustering when using these techniques, check that the model is actually recognizing this as a feature.

Example on page 79

Clustering is a core tool, that is applicable on dataset inspection or model's performance analyze.  
Example on page 81  

#### Be the Algorithm
Try to label some of the dataset yourself, while doing this you get an intuition on what you use to detect a label and what 
features a model needs to pick up, to get similar results.
**You should even do it, if your data is already labeled!**

Cycle of labeling data:
1. Separate dataset into a few meaningful clusters
2. Label a few data points per cluster
3. Identify trends and discover important predictors
4. Update data processing strategy
5. Go back to 1

To improve your labeling speed, you can start by labeling data points in each cluster you have identified and label some 
data points in each common value of a feature distribution.

#### Data Trends
If you pick up trends that have false correlations, you should do something before training the model and getting it to production, 
because it wouldn't perform well on real data.  
The best way, to deal with such a bias, is to add additional data to make your training data more representative. A worse way 
would be, to delete these features completely.

### Let Data Inform Features and Models

#### Build Features Out of Patterns
**The ease with which a model identifies patterns depends on the way we represent data and how much of it we have.**
The more data you have, the less irregularities are in them and the less feature engineering work is required.

When you start with a project it's a good idea to generate features at the beginning, because your dataset will be small anyways and
it helps to bring your on expertise into it as well.  
But when you know every weekend sales are higher than under the week, don't give your model just a date, because it would be
quite hard for you as well to determine which date is actually a weekday or not.  
In addition, dates are stored as integers, which means that an algorithm will think, that future days are "more worth", than older ones.

##### Feature crosses
feature cross features are created when multiplying (crossing) multiple features together.
Though deep neural networks can detect nonlinear combination of features, it is easier to simplify features as much as possible.

##### Giving your model the answer
If you know for a fact, that a specific combination of features is predictive, you can create a new feature, that checks if
these combinations exists in an data point (0 = false, nonzero = true)

## Robert Munro: How Do You Find, Label, and Leverage Data?

Q: How do you get started on an ML project?
A: The best way is to start with the business problem, as it will give you boundaries to
work with.

Q: How much data do you need to get started?
A: When gathering data, look for
* representativity
* diversity
* is there any unrepresented values?
* Cluster your dataset, to discover outliers.  
1,000 examples on the rarer categories work well.
At around 10,000 examples, you can trust the dataset.  
**In most cases, labeling more data yields better results than iterating on a model!**

Q: What process do you use to gather and label data?
A: There are 3 ways to improve your dataset with more data points:
* Check for cases where the model is most uncertain about and add similar examples to the training set.
* You can train an "error model", which means this second model will learn which data points the model got wrong. Now you can 
use this model on your unlabeled data and you only need to label the data it got wrong by yourself.
* create a "labeling model" which is trained on your already labeled data und unlabeled data. When trained it can detect, which 
Data from the unlabeled data has the least similarities with the labeled data. Now you know which data would be best to label next.

Q: How do you validate that your models are learning something useful?
A: To prevent to worsen the model quality it's advised to use strategies to gather data (to prevent over representation of specific data points)
and validating your model via the test set.
Other factors to check out:
* performance of model 
* uncertainty of model
* business metrics: are your usage metrics gradually going down?
